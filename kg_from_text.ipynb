{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc7b3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikipedia-api neo4j cohere  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0467cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "from time import sleep\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05301520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import fetch_raw_text, strip_gutenberg_header_footer, chunk_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89f7ce",
   "metadata": {},
   "source": [
    "### LLM API and Neo4j DB connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "750068bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohere key loaded: True\n",
      "Neo4j URI: bolt://44.200.207.55:7687\n",
      "Neo4j User: neo4j\n"
     ]
    }
   ],
   "source": [
    "# Load all keys from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "COHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "# Debug check (optional â€“ donâ€™t print secrets in real projects)\n",
    "print(\"Cohere key loaded:\", bool(COHERE_API_KEY))\n",
    "print(\"Neo4j URI:\", NEO4J_URI)\n",
    "print(\"Neo4j User:\", NEO4J_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d29efc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection test result: 1\n"
     ]
    }
   ],
   "source": [
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "#check connection\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"RETURN 1\")\n",
    "    print(\"Connection test result:\", result.single()[0])  # Should print 1 if successful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34e130",
   "metadata": {},
   "source": [
    "### Fetch Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60584fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned text -> data/sherlock_raw.txt\n"
     ]
    }
   ],
   "source": [
    "GUTENBERG_TXT_URL = \"https://www.gutenberg.org/cache/epub/244/pg244.txt\"  # A Study in Scarlet (id=244)\n",
    "#GUTENBERG_TXT_URL = \"https://www.gutenberg.org/cache/epub/18897/pg18897.txt\"  # The Epic of Gilgamish (Langdon, id=18897)\n",
    "#GUTENBERG_TXT_URL = \"https://www.gutenberg.org/cache/epub/11000/pg11000.txt\"  # An Old Babylonian Version of the Gilgamesh Epic (Jastrow & Clay, id=11000)\n",
    "\n",
    "raw = fetch_raw_text(GUTENBERG_TXT_URL)\n",
    "core = strip_gutenberg_header_footer(raw)\n",
    "\n",
    "with open(\"data/sherlock_raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(core)\n",
    "print(\"Saved cleaned text -> data/sherlock_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd2502a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 1251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the year 1878 I took my degree of Doctor of Medicine of the University of London, and proceeded to Netley to go through the course prescribed for surgeons in the army.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk Data\n",
    "text = open(\"data/sherlock_raw.txt\", encoding=\"utf-8\").read()\n",
    "\n",
    "# Clean text\n",
    "# remove /n with .\n",
    "text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"  \", \" \")\n",
    "# start text from CHAPTER I. MR. SHERLOCK HOLMES. to avoid preface\n",
    "start_idx = text.find(\"In the year 1878 I took my degree of Doctor of Medicine of the\")\n",
    "text = text[start_idx:]\n",
    "\n",
    "\n",
    "chunks = chunk_text(text, max_chars=250, overlap=150)\n",
    "print(f\"Chunks: {len(chunks)}\")\n",
    "\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfd8b1",
   "metadata": {},
   "source": [
    "### Call LLM on each chunk to identify nodes and relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "43ccf5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.ClientV2(COHERE_API_KEY, log_warning_experimental_features=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af396b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/1251\n",
      "Processing chunk 2/1251\n",
      "Processing chunk 3/1251\n",
      "Processing chunk 4/1251\n",
      "Processing chunk 5/1251\n",
      "Processing chunk 6/1251\n",
      "Processing chunk 7/1251\n",
      "Processing chunk 8/1251\n",
      "Processing chunk 9/1251\n",
      "Processing chunk 10/1251\n",
      "Processing chunk 11/1251\n",
      "Processing chunk 12/1251\n",
      "Processing chunk 13/1251\n",
      "Processing chunk 14/1251\n",
      "Processing chunk 15/1251\n",
      "Processing chunk 16/1251\n",
      "Processing chunk 17/1251\n",
      "Processing chunk 18/1251\n",
      "Processing chunk 19/1251\n",
      "Processing chunk 20/1251\n",
      "Processing chunk 21/1251\n",
      "Processing chunk 22/1251\n",
      "Processing chunk 23/1251\n",
      "Processing chunk 24/1251\n",
      "Processing chunk 25/1251\n",
      "Processing chunk 26/1251\n",
      "Processing chunk 27/1251\n",
      "Processing chunk 28/1251\n",
      "Processing chunk 29/1251\n",
      "Processing chunk 30/1251\n",
      "Processing chunk 31/1251\n",
      "Processing chunk 32/1251\n",
      "Processing chunk 33/1251\n",
      "Processing chunk 34/1251\n",
      "Processing chunk 35/1251\n",
      "Processing chunk 36/1251\n",
      "Processing chunk 37/1251\n",
      "Processing chunk 38/1251\n",
      "Processing chunk 39/1251\n",
      "Processing chunk 40/1251\n",
      "Processing chunk 41/1251\n",
      "Processing chunk 42/1251\n",
      "Processing chunk 43/1251\n",
      "Processing chunk 44/1251\n",
      "Processing chunk 45/1251\n",
      "Processing chunk 46/1251\n",
      "Processing chunk 47/1251\n",
      "Processing chunk 48/1251\n",
      "Processing chunk 49/1251\n",
      "Processing chunk 50/1251\n",
      "Processing chunk 51/1251\n",
      "Processing chunk 52/1251\n",
      "Processing chunk 53/1251\n",
      "Processing chunk 54/1251\n",
      "Processing chunk 55/1251\n",
      "Processing chunk 56/1251\n",
      "Processing chunk 57/1251\n",
      "Processing chunk 58/1251\n",
      "Processing chunk 59/1251\n",
      "Processing chunk 60/1251\n",
      "Processing chunk 61/1251\n",
      "Processing chunk 62/1251\n",
      "Processing chunk 63/1251\n",
      "Processing chunk 64/1251\n",
      "Processing chunk 65/1251\n",
      "Processing chunk 66/1251\n",
      "Processing chunk 67/1251\n",
      "Processing chunk 68/1251\n",
      "Processing chunk 69/1251\n",
      "Processing chunk 70/1251\n",
      "Processing chunk 71/1251\n",
      "Processing chunk 72/1251\n",
      "Processing chunk 73/1251\n",
      "Processing chunk 74/1251\n",
      "Processing chunk 75/1251\n",
      "Processing chunk 76/1251\n",
      "Processing chunk 77/1251\n",
      "Processing chunk 78/1251\n",
      "Processing chunk 79/1251\n",
      "Processing chunk 80/1251\n",
      "Processing chunk 81/1251\n",
      "Processing chunk 82/1251\n",
      "Processing chunk 83/1251\n",
      "Processing chunk 84/1251\n",
      "Processing chunk 85/1251\n",
      "Processing chunk 86/1251\n",
      "Processing chunk 87/1251\n",
      "Processing chunk 88/1251\n",
      "Processing chunk 89/1251\n",
      "Processing chunk 90/1251\n",
      "Processing chunk 91/1251\n",
      "Processing chunk 92/1251\n",
      "Processing chunk 93/1251\n",
      "Processing chunk 94/1251\n",
      "Processing chunk 95/1251\n",
      "Processing chunk 96/1251\n",
      "Processing chunk 97/1251\n",
      "Processing chunk 98/1251\n",
      "Processing chunk 99/1251\n",
      "Processing chunk 100/1251\n",
      "Processing chunk 101/1251\n",
      "Processing chunk 102/1251\n",
      "Processing chunk 103/1251\n",
      "Processing chunk 104/1251\n",
      "Processing chunk 105/1251\n",
      "Processing chunk 106/1251\n",
      "Processing chunk 107/1251\n",
      "Processing chunk 108/1251\n",
      "Processing chunk 109/1251\n",
      "Processing chunk 110/1251\n",
      "Processing chunk 111/1251\n",
      "Processing chunk 112/1251\n",
      "Processing chunk 113/1251\n",
      "Processing chunk 114/1251\n",
      "Processing chunk 115/1251\n",
      "Processing chunk 116/1251\n",
      "Processing chunk 117/1251\n",
      "Processing chunk 118/1251\n",
      "Processing chunk 119/1251\n",
      "Processing chunk 120/1251\n",
      "Processing chunk 121/1251\n",
      "Processing chunk 122/1251\n",
      "Processing chunk 123/1251\n",
      "Processing chunk 124/1251\n",
      "Processing chunk 125/1251\n",
      "Processing chunk 126/1251\n",
      "Processing chunk 127/1251\n",
      "Processing chunk 128/1251\n",
      "Processing chunk 129/1251\n",
      "Processing chunk 130/1251\n",
      "Processing chunk 131/1251\n",
      "Processing chunk 132/1251\n",
      "Processing chunk 133/1251\n",
      "Processing chunk 134/1251\n",
      "Processing chunk 135/1251\n",
      "Processing chunk 136/1251\n",
      "Processing chunk 137/1251\n",
      "Processing chunk 138/1251\n",
      "Processing chunk 139/1251\n",
      "Processing chunk 140/1251\n",
      "Processing chunk 141/1251\n",
      "Processing chunk 142/1251\n",
      "Processing chunk 143/1251\n",
      "Processing chunk 144/1251\n",
      "Processing chunk 145/1251\n",
      "Processing chunk 146/1251\n",
      "Processing chunk 147/1251\n",
      "Processing chunk 148/1251\n",
      "Processing chunk 149/1251\n",
      "Processing chunk 150/1251\n",
      "Processing chunk 151/1251\n",
      "Processing chunk 152/1251\n",
      "Processing chunk 153/1251\n",
      "Processing chunk 154/1251\n",
      "Processing chunk 155/1251\n",
      "Processing chunk 156/1251\n",
      "Processing chunk 157/1251\n",
      "Processing chunk 158/1251\n",
      "Processing chunk 159/1251\n",
      "Processing chunk 160/1251\n",
      "Processing chunk 161/1251\n",
      "Processing chunk 162/1251\n",
      "Processing chunk 163/1251\n",
      "Processing chunk 164/1251\n",
      "Processing chunk 165/1251\n",
      "Processing chunk 166/1251\n",
      "Processing chunk 167/1251\n",
      "Processing chunk 168/1251\n",
      "Processing chunk 169/1251\n",
      "Processing chunk 170/1251\n",
      "Processing chunk 171/1251\n",
      "Processing chunk 172/1251\n",
      "Processing chunk 173/1251\n",
      "Processing chunk 174/1251\n",
      "Processing chunk 175/1251\n",
      "Processing chunk 176/1251\n",
      "Processing chunk 177/1251\n",
      "Processing chunk 178/1251\n",
      "Processing chunk 179/1251\n",
      "Processing chunk 180/1251\n",
      "Processing chunk 181/1251\n",
      "Processing chunk 182/1251\n",
      "Processing chunk 183/1251\n",
      "Processing chunk 184/1251\n",
      "Processing chunk 185/1251\n",
      "Processing chunk 186/1251\n",
      "Processing chunk 187/1251\n",
      "Processing chunk 188/1251\n",
      "Processing chunk 189/1251\n",
      "Processing chunk 190/1251\n",
      "Processing chunk 191/1251\n",
      "Processing chunk 192/1251\n",
      "Processing chunk 193/1251\n",
      "Processing chunk 194/1251\n",
      "Processing chunk 195/1251\n",
      "Processing chunk 196/1251\n",
      "Processing chunk 197/1251\n",
      "Processing chunk 198/1251\n",
      "Processing chunk 199/1251\n",
      "Processing chunk 200/1251\n",
      "Processing chunk 201/1251\n",
      "Processing chunk 202/1251\n",
      "Processing chunk 203/1251\n",
      "Processing chunk 204/1251\n",
      "Processing chunk 205/1251\n",
      "Processing chunk 206/1251\n",
      "Processing chunk 207/1251\n",
      "Processing chunk 208/1251\n",
      "Processing chunk 209/1251\n",
      "Processing chunk 210/1251\n",
      "Processing chunk 211/1251\n",
      "Processing chunk 212/1251\n",
      "Processing chunk 213/1251\n",
      "Processing chunk 214/1251\n",
      "Processing chunk 215/1251\n",
      "Processing chunk 216/1251\n",
      "Processing chunk 217/1251\n",
      "Processing chunk 218/1251\n",
      "Processing chunk 219/1251\n",
      "Processing chunk 220/1251\n",
      "Processing chunk 221/1251\n",
      "Processing chunk 222/1251\n",
      "Processing chunk 223/1251\n",
      "Processing chunk 224/1251\n",
      "Processing chunk 225/1251\n",
      "Processing chunk 226/1251\n",
      "Processing chunk 227/1251\n",
      "Processing chunk 228/1251\n",
      "Processing chunk 229/1251\n",
      "Processing chunk 230/1251\n",
      "Processing chunk 231/1251\n",
      "Processing chunk 232/1251\n",
      "Processing chunk 233/1251\n",
      "Processing chunk 234/1251\n",
      "Processing chunk 235/1251\n",
      "Processing chunk 236/1251\n",
      "Processing chunk 237/1251\n",
      "Processing chunk 238/1251\n",
      "Processing chunk 239/1251\n",
      "Processing chunk 240/1251\n",
      "Processing chunk 241/1251\n",
      "Processing chunk 242/1251\n",
      "Processing chunk 243/1251\n",
      "Processing chunk 244/1251\n",
      "Processing chunk 245/1251\n",
      "Processing chunk 246/1251\n",
      "Processing chunk 247/1251\n",
      "Processing chunk 248/1251\n",
      "Processing chunk 249/1251\n",
      "Processing chunk 250/1251\n",
      "Processing chunk 251/1251\n",
      "Processing chunk 252/1251\n",
      "Processing chunk 253/1251\n",
      "Processing chunk 254/1251\n",
      "Processing chunk 255/1251\n",
      "Processing chunk 256/1251\n",
      "Processing chunk 257/1251\n",
      "Processing chunk 258/1251\n",
      "Processing chunk 259/1251\n",
      "Processing chunk 260/1251\n",
      "Processing chunk 261/1251\n",
      "Processing chunk 262/1251\n",
      "Processing chunk 263/1251\n",
      "Processing chunk 264/1251\n",
      "Processing chunk 265/1251\n",
      "Processing chunk 266/1251\n",
      "Processing chunk 267/1251\n",
      "Processing chunk 268/1251\n",
      "Processing chunk 269/1251\n",
      "Processing chunk 270/1251\n",
      "Processing chunk 271/1251\n",
      "Processing chunk 272/1251\n",
      "Processing chunk 273/1251\n",
      "Processing chunk 274/1251\n",
      "Processing chunk 275/1251\n",
      "Processing chunk 276/1251\n",
      "Processing chunk 277/1251\n",
      "Processing chunk 278/1251\n",
      "Processing chunk 279/1251\n",
      "Processing chunk 280/1251\n",
      "Processing chunk 281/1251\n",
      "Processing chunk 282/1251\n",
      "Processing chunk 283/1251\n",
      "Processing chunk 284/1251\n",
      "Processing chunk 285/1251\n",
      "Processing chunk 286/1251\n",
      "Processing chunk 287/1251\n",
      "Processing chunk 288/1251\n",
      "Processing chunk 289/1251\n",
      "Processing chunk 290/1251\n",
      "Processing chunk 291/1251\n",
      "Processing chunk 292/1251\n",
      "Processing chunk 293/1251\n",
      "Processing chunk 294/1251\n",
      "Processing chunk 295/1251\n",
      "Processing chunk 296/1251\n",
      "Processing chunk 297/1251\n",
      "Processing chunk 298/1251\n",
      "Processing chunk 299/1251\n",
      "Processing chunk 300/1251\n",
      "Processing chunk 301/1251\n",
      "Processing chunk 302/1251\n",
      "Processing chunk 303/1251\n",
      "Processing chunk 304/1251\n",
      "Processing chunk 305/1251\n",
      "Processing chunk 306/1251\n",
      "Processing chunk 307/1251\n",
      "Processing chunk 308/1251\n",
      "Processing chunk 309/1251\n",
      "Processing chunk 310/1251\n",
      "Processing chunk 311/1251\n",
      "Processing chunk 312/1251\n",
      "Processing chunk 313/1251\n",
      "Processing chunk 314/1251\n",
      "Processing chunk 315/1251\n",
      "Processing chunk 316/1251\n",
      "Processing chunk 317/1251\n",
      "Processing chunk 318/1251\n",
      "Processing chunk 319/1251\n",
      "Processing chunk 320/1251\n",
      "Processing chunk 321/1251\n",
      "Processing chunk 322/1251\n",
      "Processing chunk 323/1251\n",
      "Processing chunk 324/1251\n",
      "Processing chunk 325/1251\n",
      "Processing chunk 326/1251\n",
      "Processing chunk 327/1251\n",
      "Processing chunk 328/1251\n",
      "Processing chunk 329/1251\n",
      "Processing chunk 330/1251\n",
      "Processing chunk 331/1251\n",
      "Processing chunk 332/1251\n",
      "Processing chunk 333/1251\n",
      "Processing chunk 334/1251\n",
      "Processing chunk 335/1251\n",
      "Processing chunk 336/1251\n",
      "Processing chunk 337/1251\n",
      "Processing chunk 338/1251\n",
      "Processing chunk 339/1251\n",
      "Processing chunk 340/1251\n",
      "Processing chunk 341/1251\n",
      "Processing chunk 342/1251\n",
      "Processing chunk 343/1251\n",
      "Processing chunk 344/1251\n",
      "Processing chunk 345/1251\n",
      "Processing chunk 346/1251\n",
      "Processing chunk 347/1251\n",
      "Processing chunk 348/1251\n",
      "Processing chunk 349/1251\n",
      "Processing chunk 350/1251\n",
      "Processing chunk 351/1251\n",
      "Processing chunk 352/1251\n",
      "Processing chunk 353/1251\n",
      "Processing chunk 354/1251\n",
      "Processing chunk 355/1251\n",
      "Processing chunk 356/1251\n",
      "Processing chunk 357/1251\n",
      "Processing chunk 358/1251\n",
      "Processing chunk 359/1251\n",
      "Processing chunk 360/1251\n",
      "Processing chunk 361/1251\n",
      "Processing chunk 362/1251\n",
      "Processing chunk 363/1251\n",
      "Processing chunk 364/1251\n",
      "Processing chunk 365/1251\n",
      "Processing chunk 366/1251\n",
      "Processing chunk 367/1251\n",
      "Processing chunk 368/1251\n",
      "Processing chunk 369/1251\n",
      "Processing chunk 370/1251\n",
      "Processing chunk 371/1251\n",
      "Processing chunk 372/1251\n",
      "Processing chunk 373/1251\n",
      "Processing chunk 374/1251\n",
      "Processing chunk 375/1251\n",
      "Processing chunk 376/1251\n",
      "Processing chunk 377/1251\n",
      "Processing chunk 378/1251\n",
      "Processing chunk 379/1251\n",
      "Processing chunk 380/1251\n",
      "Processing chunk 381/1251\n",
      "Processing chunk 382/1251\n",
      "Processing chunk 383/1251\n",
      "Processing chunk 384/1251\n",
      "Processing chunk 385/1251\n",
      "Processing chunk 386/1251\n",
      "Processing chunk 387/1251\n",
      "Processing chunk 388/1251\n",
      "Processing chunk 389/1251\n",
      "Processing chunk 390/1251\n",
      "Processing chunk 391/1251\n",
      "Processing chunk 392/1251\n",
      "Processing chunk 393/1251\n",
      "Processing chunk 394/1251\n",
      "Processing chunk 395/1251\n",
      "Processing chunk 396/1251\n",
      "Processing chunk 397/1251\n",
      "Processing chunk 398/1251\n",
      "Processing chunk 399/1251\n",
      "Processing chunk 400/1251\n",
      "Processing chunk 401/1251\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1310 column 14 (char 25711)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 39\u001b[0m\n\u001b[0;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mchat(\n\u001b[0;32m     30\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommand-a-03-2025\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     }\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Parse model output\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m sleep(\u001b[38;5;241m7\u001b[39m)  \u001b[38;5;66;03m# avoid rate limits\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# ðŸ”¹ Merge entities\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\salmank\\anaconda3\\envs\\pymc_env_5\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1310 column 14 (char 25711)"
     ]
    }
   ],
   "source": [
    "# Global lists\n",
    "global_entities = []\n",
    "global_entity_map = {}      # name/alias -> id\n",
    "global_relationships = []\n",
    "global_relation_types = set()  # unique relation types\n",
    "existing_rels = set()       # (source_id, relation_type, target_id) tuples\n",
    "entity_counter = 1\n",
    "\n",
    "# Load the response schema\n",
    "with open(\"response_schema.json\") as f:\n",
    "    response_schema = json.load(f)\n",
    "\n",
    "# Loop over each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "    # Prepare global lists\n",
    "    entity_list_str = json.dumps({\"entities\": global_entities}, ensure_ascii=False)\n",
    "    relation_list_str = json.dumps(list(global_relation_types), ensure_ascii=False)\n",
    "\n",
    "    # Load prompt template\n",
    "    prompt = open(\"prompt_template.txt\").read()\n",
    "    #prompt = open(\"prompt_v2.txt\").read()\n",
    "    prompt = prompt.replace(\"{CHUNK}\", chunk)\n",
    "    prompt = prompt.replace(\"{ENTITYLIST}\", entity_list_str)\n",
    "    prompt = prompt.replace(\"{RELATIONLIST}\", relation_list_str)\n",
    "\n",
    "    # Call the LLM\n",
    "    response = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": response_schema\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Parse model output\n",
    "    data = json.loads(response.dict()[\"message\"][\"content\"][0][\"text\"])\n",
    "    sleep(7)  # avoid rate limits\n",
    "\n",
    "    # ðŸ”¹ Merge entities\n",
    "    for ent in data[\"entities\"]:\n",
    "        key = ent[\"name\"].lower()\n",
    "        if key in global_entity_map:\n",
    "            ent[\"id\"] = global_entity_map[key]\n",
    "        else:\n",
    "            ent_id = f\"e{entity_counter}\"\n",
    "            ent[\"id\"] = ent_id\n",
    "            global_entity_map[key] = ent_id\n",
    "            global_entities.append(ent)\n",
    "            entity_counter += 1\n",
    "\n",
    "    # ðŸ”¹ Merge relationships (deduplicate and normalize)\n",
    "    for rel in data[\"relationships\"]:\n",
    "        src_id = global_entity_map.get(rel[\"source\"].lower(), rel[\"source\"])\n",
    "        tgt_id = global_entity_map.get(rel[\"target\"].lower(), rel[\"target\"])\n",
    "        rel_type = rel[\"relation\"].lower()\n",
    "\n",
    "        rel_key = (src_id, rel_type, tgt_id)\n",
    "        if rel_key not in existing_rels:\n",
    "            rel[\"source\"] = src_id\n",
    "            rel[\"target\"] = tgt_id\n",
    "            rel[\"relation\"] = rel_type\n",
    "            global_relationships.append(rel)\n",
    "            existing_rels.add(rel_key)\n",
    "            global_relation_types.add(rel_type)\n",
    "\n",
    "# ðŸ”¹ Final merged JSON\n",
    "final_output = {\n",
    "    \"entities\": global_entities,\n",
    "    \"relationships\": global_relationships\n",
    "}\n",
    "\n",
    "# Save or print\n",
    "print(json.dumps(final_output, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c28f1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(final_output, indent=2, ensure_ascii=False))\n",
    "#save to file\n",
    "with open(\"extracted.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_output, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e9eaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your extracted JSON\n",
    "with open(\"extracted.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209816c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94a93702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All nodes and relationships have been deleted.\n"
     ]
    }
   ],
   "source": [
    "delete_query = \"MATCH (n) DETACH DELETE n\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(delete_query)\n",
    "\n",
    "print(\"All nodes and relationships have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c62b77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nodes with dynamic labels\n",
    "query_nodes = \"\"\"\n",
    "UNWIND $entities AS entity\n",
    "CALL apoc.merge.node([entity.type], {id: entity.id}, \n",
    "                     {name: entity.name, aliases: entity.aliases, span: entity.span}, \n",
    "                     {}) YIELD node\n",
    "RETURN node\n",
    "\"\"\"\n",
    "\n",
    "# Create relationships with dynamic types\n",
    "query_rels = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (src {id: rel.source})\n",
    "MATCH (tgt {id: rel.target})\n",
    "CALL apoc.merge.relationship(src, rel.relation, {}, {evidence: rel.evidence_span}, tgt) YIELD rel AS r\n",
    "RETURN r\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(query_nodes, entities=data[\"entities\"])\n",
    "    session.run(query_rels, relationships=data[\"relationships\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93047ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
